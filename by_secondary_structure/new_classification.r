library(dplyr)
library(caret)
library(ROCR)
library(plotROC)
library(ggplot2)
library(plotly)
library(corrplot)

### QUICK START
setwd('C:/Users/Jan_Huang/Desktop/helix_grindtime/by_secondary_structure')
data <- read.csv('data_full.csv')
data <- subset(data, select=c('seq', 'code', 'disp', 'GC', 'ss', 'maxORF', 'ORFcover', 'maxnstop'))
data$code <- as.factor(data$code)

# make results reproducible
set.seed(500)

### INITIAL DATASET CLEANING

# filtering by length is done in Python

# make datasets the same size by randomly choosing sequences from bigger dataset
fraction <- nrow(coding) / nrow(noncoding)
nc_index <- sample(x = 1:nrow(noncoding), size = round(fraction * nrow(noncoding)))
noncoding <- noncoding[nc_index, ]

### FEATURE EXTRACTION
# values imported from files generated by Python algorithms

disp <- as.matrix(read.csv('data_full_disp.csv'))
for (i in 1:nrow(disp)) {
  data$disp[i] <- disp[i]
}
GC <- as.matrix(read.csv('data_full_GC.csv'))
for (i in 1:nrow(GC)) {
  data$GC[i] <- GC[i]
}
maxORF <- as.matrix(read.csv('data_full_maxORF.csv'))
for (i in 1:nrow(maxORF)) {
  data$maxORF[i] <- maxORF[i]
}
ORFcover<- as.matrix(read.csv('data_full_ORFcover.csv'))
for (i in 1:nrow(ORFcover)) {
  data$ORFcover[i] <- ORFcover[i]
}
maxNStop <- as.matrix(read.csv('data_full_maxNStop.csv'))
for (i in 1:nrow(maxNStop)) {
  data$maxnstop[i] <- maxNStop[i]
}
write.csv(data, 'data_full.csv')

# remove duplicate sequences
data <- data[!duplicated(data$seq), ]

### DESCRIPTIVE STATISTICS FOR FEATURES

# compare sequence lengths
for (i in 1:nrow(coding)) {
  coding$length[i] <- nchar(as.character(coding$seq[i]))
}
for (i in 1:nrow(noncoding)) {
  noncoding$length[i] <- nchar(as.character(noncoding$seq[i]))
}

hist(coding$length, main='Length of Coding Sequences', xlim=c(0, 100000), breaks=100, xlab='Length')
hist(noncoding$length, main='Length of Noncoding Sequences', xlim=c(0, 100000), breaks=100, xlab='Length')

# correlation matrix
feature_cols <- c(3,4,6,7,8)
M <- cor(data[feature_cols])
corrplot(M, type='upper', method='color', tl.col='black', tl.srt=45, addCoef.col='black', diag=FALSE, mar=c(0,0,1,0))

mean(data[data$code == '1', ]$GC) # 51.86%
mean(data[data$code == '0', ]$GC) # 46.49%
sd(data[data$code == '1', ]$GC) # 7.93%
sd(data[data$code == '0', ]$GC) # 12.02%

mean(data[data$code == '1', ]$disp) # .73
mean(data[data$code == '0', ]$disp) # .61
sd(data[data$code == '1', ]$disp) # .11
sd(data[data$code == '0', ]$disp) # .13

mean(data[data$code == '1', ]$maxORF) # 439.11
mean(data[data$code == '0', ]$maxORF) # 221.08
sd(data[data$code == '1', ]$maxORF) # 394.13
sd(data[data$code == '0', ]$maxORF) # 230.42

shapiro.test(data[data$code=='1',]$GC) # not normal; p=1.028e-07
shapiro.test(data[data$code=='0',]$GC) # not normal; p=6.503e-05
shapiro.test(data[data$code=='1',]$disp) # not normal; p=2.241e-07
shapiro.test(data[data$code=='0',]$disp) # not normal; p=2.2e-16
# conclusion: none are normal

fligner.test(data[data$code=='1',]$GC, data[data$code=='0',]$GC) # p=.405, variances are same
fligner.test(data[data$code=='1',]$disp, data[data$code=='0',]$disp) # p=.492, variances are same
# conclusion: variances are same

# Wilcoxon Rank Sum Test (compare mean of 2 samples)
# alternative: true location shift is not equal to 0
wilcox.test(data[data$code=='1',]$GC, data[data$code=='0',]$GC) # p=9.46e-14
wilcox.test(data[data$code=='1',]$disp, data[data$code=='0',]$disp) # p=2.2e-16
# conclusion: from diff distributions

ggplot(data=data, aes(data$disp, fill=code)) + geom_histogram(binwidth = .05) + labs(title='Histogram of Median Disparity', x='Median Disparity', y='Count') + scale_fill_discrete(name = '', labels = c('Noncoding', 'Coding'))

# by SS!
ggplot(data=data, aes(x=data$disp, y=..density.., color=code, fill=ss)) +
  xlim(0.3,1.2) +
  scale_fill_discrete(name = 'Secondary Structure', breaks = c('', 'a', 'ab', 'b', 'fSS'), labels = c('Noncoding', 'Mainly Alpha', 'Alpha Beta', 'Mainly Beta', 'Few Secondary Structures')) +
  labs(title='Median Disparity by Secondary Structure', x='Median Disparity', y='Density') +
  geom_density(alpha=0.1)

ggplot(data=data, aes(x=data$GC, y=..density.., color=code, fill=code)) +
  scale_fill_discrete(name = '', labels = c('Noncoding', 'Coding')) +
  labs(title='GC Percentage vs Coding Status', x='GC%', y='Density') +
  geom_density(alpha=0.1)

plot_ly(data=data, x=~GC, y=~disp, z=~maxORF, color=~code)

ggplot(data=data, aes(x=data$disp, y=..density.., color=ss, fill=ss)) + labs(title='Median Disparity', x='Median Disparity', y='Density') + geom_density(position='fill')

ggplot(data=data, aes(data$GC, fill=code)) + geom_histogram(binwidth = 1.15) + labs(title='Histogram of GC Percentage', x='GC Percentage', y='Count') + scale_fill_discrete(name = '', labels = c('Noncoding', 'Coding'))

ggplot(data=data, aes(data$maxORF, fill=code)) + geom_histogram() + labs(title='Histogram of Max ORF Length', x='Max ORF Length', y='Count') + scale_fill_discrete(name = '', labels = c('Noncoding', 'Coding'))

# visualize correlations btwn/among features
ggplot(data=data, aes(x=data$GC, y=data$disp)) + geom_point(aes(GC, disp, color=code)) + labs(title='Scatterplot of GC Percentage vs Median Disparity')

ggplot(data=data, aes(x=data$maxORF, y=data$disp)) + geom_point(aes(GC, disp, color=code)) + labs(title='Scatterplot')

### 75/25 TRAIN/TEST PROPORTIONAL SPLIT

# proportional split
train_indices_a <- sample(x = as.vector(which(data$ss=='a')), size = round(0.75 * nrow(data[data$ss=='a', ])))
train_indices_b <- sample(x = as.vector(which(data$ss=='b')), size = round(0.75 * nrow(data[data$ss=='b', ])))
train_indices_ab <- sample(x = as.vector(which(data$ss=='ab')), size = round(0.75 * nrow(data[data$ss=='ab', ])))
train_indices_fSS <- sample(x = as.vector(which(data$ss=='fSS')), size = round(0.75 * nrow(data[data$ss=='fSS', ])))
train_indices_nc <- sample(x = as.vector(which(data$code==0)), size = round(0.75 * nrow(data[data$code==0, ])))

train_indices_all <- c(train_indices_a, train_indices_b, train_indices_ab, train_indices_fSS, train_indices_nc)
train <- data[train_indices_all,]
test <- data[-train_indices_all,]

### CROSS VALIDATION

### LOGISTIC REGRESSION

# train model on GC + disp
model <- glm(code ~ GC + disp, family = 'binomial', train)
p <- predict(model, test, type='response')
summary(p)
#      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
# 0.0003895 0.3029141 0.4716623 0.4928045 0.6545059 0.9922203
p_class <- ifelse(p > .50, 1, 0)
confusionMatrix(p_class, test[['code']])
# accuracy: 75.08%
# 95% CI: (69.9%, 79.8%)
# sens: 76.51%
# spec: %73.47

write.csv(train, 'data_train_proportional.csv')
write.csv(test, 'data_test_proportional.csv')

# train model on GC, disp, maxORF, and ORFcover
model <- glm(code ~ GC + disp + maxORF + ORFcover, family = 'binomial', train)
p <- predict(model, test, type='response')
summary(p)
#    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
#0.006836 0.899453 0.954246 0.915833 0.982934 1.000000
p_class <- ifelse(p > .50, 1, 0)
confusionMatrix(p_class, test[['code']])
# accuracy: 75.08%
# 95% CI: (69.9%, 79.8%)
# sens: 76.51%
# spec: %73.47

# train model on GC, disp, maxORF, ORFcover, maxnstop
model <- glm(code ~ GC + disp + maxORF + ORFcover + maxnstop, family = 'binomial', train)
p <- predict(model, test, type='response')
summary(p)
#    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
#0.006836 0.899453 0.954246 0.915833 0.982934 1.000000
p_class <- ifelse(p > .50, 1, 0)
confusionMatrix(p_class, test[['code']])
# accuracy: 75.08%
# 95% CI: (69.9%, 79.8%)
# sens: 76.51%
# spec: %73.47

### ROC CURVE
pred <- prediction(p, test$code)
sens <- performance(pred, measure='sens', x.measure='cutoff') # true positive rate
spec <- performance(pred, measure='spec', x.measure='cutoff') # true negative rate
plot(sens, main='Sensitivity and Specificity vs Cutoff', col='red', ylab='Sensitivity/Specificity')
plot(spec, add=TRUE, col='blue')
# work out geom_roc later

### RANK FEATURES BY IMPORTANCE
# if feature less important than random noise, then it's not significant
# uses learning vector quantization (LVQ)
importance <- varImp(model, scale=FALSE)
print(importance)
#       Overall
#GC    5.570123
#disp 12.255953